---
layout: post
title:  "ViT"
date:   2023-07-28 13:56:33 +0900
categories: jekyll update
use_math: true
---

-----------------------------


# [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

# ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled.png)

# [paper review] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
## Introduction
- self-attention과 CNN을 mix한 기존의 연구들과 달리, vanilla transformer를 image patch에 적용, classification task에서 좋은 성능을 내고 심지어는 SOTA에 비견할 수 있음을 밝힘

## ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled.png)

## ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled%201.png)

## Method
- 위 그림에서 보이듯, image를 patch 단위로 자른 후 flattening하고, 각 patch를 projection한 후 positional embedding을 더함. 이후로는 각 patch를 하나의 token으로 간주하며 BERT와 유사하게 Transformer Encoder를 훈련시킬 것.
- self-attention을 global하게 진행하는데, 어떻게 locality를 학습하나?
    - 처음에 image를 patch 단위로 자르고 flattening하는 과정
    - 그 직후 position embedding을 더하는 과정
- input sequence를 raw image에 대해 CNN을 수행한 feature map으로 하여 hybrid architecture를 구축하는 방법도 있음
- higher resolution을 가지는 image를 training할 때는 patch size를 유지하고 sequence length를 늘림
## Experiments
- Training
    - dataset, benchmarks → 알고 싶다면^^
        - **dataset**: ILSVRC-2012 ImageNet dataset
        - **benchmarks**: cleaned-up ReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al., 2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008)
        - evaluation on 19-task VTAB classification suite
            - evaluates low-data transfer to diverse tasks, using 1 000 training examples per task
            - Three groups: Natural / Specialized (medical and satellite imagery) / Structured (require geometric understanding like localization)
    - Model variants: ViT - Base, Large, Huge

        ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled%202.png)

        ViT-L/16 ← “Large” variant with 16x16 input patch size

    - Baseline: ResNet (BiT)
        - modifications
            - replace Batch Normalization with Group Normalization
            - used standardized convolutions
- results
    - Comparison with SOTA

        ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled%203.png)

        ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled%204.png)

    - Inspecting Vision Transformer

        ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled%205.png)

        - layer1: flattened patch를 projection함

            그림 1 → embedding filter들의 principal components를 시각화

        - layer2: position embedding을 추가함

            그림 2 → image 내부의 distance가 model에 잘 encoding된다는 증거. closer patch가 더 유사한 positional embedding을 가짐.

            (Appendix D, Appendix D.4 참고)

        - 그림 3 → ‘attention distance’, 즉 image space에서 self-attention을 통해 information integration이 일어난 average distance를 계산함.
            - highly localized attention을 보았을 때, CNN의 convolution layer과 유사한 기능을 ViT가 수행할 수 있다는 점을 새롭게 시사함.
            - network depth를 늘릴수록 attention distance가 증가함
        - ViT가 classification을 위해 semantically relevant한 image region에 대해 attend할 수 있음

            ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled%206.png)

## Conclusion
- summary
    - Image를 patch들의 sequence로 간주하고 standard NLP (특히 BERT)와 같은 방법으로 처리해봤는데, SOTA에 버금가는 성능을 내고 pre-train하기에 cost가 덜함.
- future works
    - detection, segmentation 같은 다른 task에도 적용해볼 것
    - self-supervised training method를 적용해볼 것 (BERT 같은 masked language modeling을 모방하여 masked patch prediction을 시도할 것)
# [code review] https://github.com/google-research/vision_transformer
## [models_resnet.py](https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_resnet.py)
- `StdConv`: convolution인데 weight을 standardize함
- `ResidualUnit`: StdConv-Groupnorm-StdConv-Groupnorm-relu-StdConv-Groupnorm-relu-StdConv-Groupnorm-relu
- `ResNetStage`: ResidualUnit여러번
## [models_vit.py](https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_vit.py)
- `AddPositionEmbs`: param을 input에 더하는 layer
- `MlpBlock`: dense-gelu-dropout-dense-dropout (feed-forward)
- `Encoder1DBlock`: layernorm-multiHeadAttention-dropout-(skipconnection)-layernorm-MlpBlock-(skipconnection)

    ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled%207.png)

- `Encoder`: AddPositionEmbs-dropout-Encoder1dBlock여러번-layernorm
- **`VisionTransformer`**: stdConv-Groupnorm-relu-maxpooling-ResNetStage여러번-conv-TransformerEncoder-dense-tanh-dense(class개수로)

## <aside>
## 💡 google_research repo인데 pretrained checkpoint도 있다!

## </aside>


# CufftY (Yoonah Park)
## BIO
----------
Undergraduate Student majoring Computer Science & Engineering, Interested in Cognitive Architecture, Cellular Automata, and other DL, ML branches of study.

## Organization
----------
Seoul National University, Dept. of Computer Science & Engineering

AttentionX

## Contact
----------
[E-mail](wisdomsword21@snu.ac.kr)

[GitHub](https://github.com/gyuuuna)