---
layout: post
title:  "ViT"
date:   2023-07-28 13:56:33 +0900
categories: jekyll update
use_math: true
---

-----------------------------


# [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)

# ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled.png)

# [paper review] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
## Introduction
- self-attentionê³¼ CNNì„ mixí•œ ê¸°ì¡´ì˜ ì—°êµ¬ë“¤ê³¼ ë‹¬ë¦¬, vanilla transformerë¥¼ image patchì— ì ìš©, classification taskì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ê³  ì‹¬ì§€ì–´ëŠ” SOTAì— ë¹„ê²¬í•  ìˆ˜ ìˆìŒì„ ë°í˜

## ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled.png)

## ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled%201.png)

## Method
- ìœ„ ê·¸ë¦¼ì—ì„œ ë³´ì´ë“¯, imageë¥¼ patch ë‹¨ìœ„ë¡œ ìë¥¸ í›„ flatteningí•˜ê³ , ê° patchë¥¼ projectioní•œ í›„ positional embeddingì„ ë”í•¨. ì´í›„ë¡œëŠ” ê° patchë¥¼ í•˜ë‚˜ì˜ tokenìœ¼ë¡œ ê°„ì£¼í•˜ë©° BERTì™€ ìœ ì‚¬í•˜ê²Œ Transformer Encoderë¥¼ í›ˆë ¨ì‹œí‚¬ ê²ƒ.
- self-attentionì„ globalí•˜ê²Œ ì§„í–‰í•˜ëŠ”ë°, ì–´ë–»ê²Œ localityë¥¼ í•™ìŠµí•˜ë‚˜?
    - ì²˜ìŒì— imageë¥¼ patch ë‹¨ìœ„ë¡œ ìë¥´ê³  flatteningí•˜ëŠ” ê³¼ì •
    - ê·¸ ì§í›„ position embeddingì„ ë”í•˜ëŠ” ê³¼ì •
- input sequenceë¥¼ raw imageì— ëŒ€í•´ CNNì„ ìˆ˜í–‰í•œ feature mapìœ¼ë¡œ í•˜ì—¬ hybrid architectureë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ë„ ìˆìŒ
- higher resolutionì„ ê°€ì§€ëŠ” imageë¥¼ trainingí•  ë•ŒëŠ” patch sizeë¥¼ ìœ ì§€í•˜ê³  sequence lengthë¥¼ ëŠ˜ë¦¼
## Experiments
- Training
    - dataset, benchmarks â†’ ì•Œê³  ì‹¶ë‹¤ë©´^^
        - **dataset**: ILSVRC-2012 ImageNet dataset
        - **benchmarks**: cleaned-up ReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al., 2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008)
        - evaluation on 19-task VTAB classification suite
            - evaluates low-data transfer to diverse tasks, using 1 000 training examples per task
            - Three groups: Natural / Specialized (medical and satellite imagery) / Structured (require geometric understanding like localization)
    - Model variants: ViT - Base, Large, Huge

        ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled%202.png)

        ViT-L/16 â† â€œLargeâ€ variant with 16x16 input patch size

    - Baseline: ResNet (BiT)
        - modifications
            - replace Batch Normalization with Group Normalization
            - used standardized convolutions
- results
    - Comparison with SOTA

        ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled%203.png)

        ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled%204.png)

    - Inspecting Vision Transformer

        ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled%205.png)

        - layer1: flattened patchë¥¼ projectioní•¨

            ê·¸ë¦¼ 1 â†’ embedding filterë“¤ì˜ principal componentsë¥¼ ì‹œê°í™”

        - layer2: position embeddingì„ ì¶”ê°€í•¨

            ê·¸ë¦¼ 2 â†’ image ë‚´ë¶€ì˜ distanceê°€ modelì— ì˜ encodingëœë‹¤ëŠ” ì¦ê±°. closer patchê°€ ë” ìœ ì‚¬í•œ positional embeddingì„ ê°€ì§.

            (Appendix D, Appendix D.4 ì°¸ê³ )

        - ê·¸ë¦¼ 3 â†’ â€˜attention distanceâ€™, ì¦‰ image spaceì—ì„œ self-attentionì„ í†µí•´ information integrationì´ ì¼ì–´ë‚œ average distanceë¥¼ ê³„ì‚°í•¨.
            - highly localized attentionì„ ë³´ì•˜ì„ ë•Œ, CNNì˜ convolution layerê³¼ ìœ ì‚¬í•œ ê¸°ëŠ¥ì„ ViTê°€ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ìƒˆë¡­ê²Œ ì‹œì‚¬í•¨.
            - network depthë¥¼ ëŠ˜ë¦´ìˆ˜ë¡ attention distanceê°€ ì¦ê°€í•¨
        - ViTê°€ classificationì„ ìœ„í•´ semantically relevantí•œ image regionì— ëŒ€í•´ attendí•  ìˆ˜ ìˆìŒ

            ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled%206.png)

## Conclusion
- summary
    - Imageë¥¼ patchë“¤ì˜ sequenceë¡œ ê°„ì£¼í•˜ê³  standard NLP (íŠ¹íˆ BERT)ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ì²˜ë¦¬í•´ë´¤ëŠ”ë°, SOTAì— ë²„ê¸ˆê°€ëŠ” ì„±ëŠ¥ì„ ë‚´ê³  pre-trainí•˜ê¸°ì— costê°€ ëœí•¨.
- future works
    - detection, segmentation ê°™ì€ ë‹¤ë¥¸ taskì—ë„ ì ìš©í•´ë³¼ ê²ƒ
    - self-supervised training methodë¥¼ ì ìš©í•´ë³¼ ê²ƒ (BERT ê°™ì€ masked language modelingì„ ëª¨ë°©í•˜ì—¬ masked patch predictionì„ ì‹œë„í•  ê²ƒ)
# [code review] https://github.com/google-research/vision_transformer
## [models_resnet.py](https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_resnet.py)
- `StdConv`: convolutionì¸ë° weightì„ standardizeí•¨
- `ResidualUnit`: StdConv-Groupnorm-StdConv-Groupnorm-relu-StdConv-Groupnorm-relu-StdConv-Groupnorm-relu
- `ResNetStage`: ResidualUnitì—¬ëŸ¬ë²ˆ
## [models_vit.py](https://github.com/google-research/vision_transformer/blob/main/vit_jax/models_vit.py)
- `AddPositionEmbs`: paramì„ inputì— ë”í•˜ëŠ” layer
- `MlpBlock`: dense-gelu-dropout-dense-dropout (feed-forward)
- `Encoder1DBlock`: layernorm-multiHeadAttention-dropout-(skipconnection)-layernorm-MlpBlock-(skipconnection)

    ![Untitled](https://agency301.github.io/assets/img/ViT/Untitled%207.png)

- `Encoder`: AddPositionEmbs-dropout-Encoder1dBlockì—¬ëŸ¬ë²ˆ-layernorm
- **`VisionTransformer`**: stdConv-Groupnorm-relu-maxpooling-ResNetStageì—¬ëŸ¬ë²ˆ-conv-TransformerEncoder-dense-tanh-dense(classê°œìˆ˜ë¡œ)

## <aside>
## ğŸ’¡ google_research repoì¸ë° pretrained checkpointë„ ìˆë‹¤!

## </aside>


# CufftY (Yoonah Park)
## BIO
----------
Undergraduate Student majoring Computer Science & Engineering, Interested in Cognitive Architecture, Cellular Automata, and other DL, ML branches of study.

## Organization
----------
Seoul National University, Dept. of Computer Science & Engineering

AttentionX

## Contact
----------
[E-mail](wisdomsword21@snu.ac.kr)

[GitHub](https://github.com/gyuuuna)