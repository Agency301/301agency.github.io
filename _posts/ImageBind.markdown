
# [ImageBind: One Embedding Space To Bind Them All](https://arxiv.org/abs/2305.05665)

# Introduction
## ImageBindëŠ” image, text, audio, depth, thermal, IMU dataì˜ 6ê°€ì§€ modalitiesì— ëŒ€í•˜ì—¬ joint embeddingì„ ë§Œë“¤ê¸° ìœ„í•œ ì‹œë„ë‹¤.
## ë…íŠ¹í•œ ì ì€ image alignmentë§Œì„ ì‚¬ìš©í•˜ì—¬ joint embedding spaceë¥¼ trainingí•  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ì œì•ˆí•œ ê²ƒ.
## ëª¨ë“  ì¡°í•©ì˜ paired dataë¥¼ ëª¨ë‘ ì´ìš©í•  í•„ìš” ì—†ì´ image-paired dataë§Œì„ ì´ìš©í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶©ë¶„í•˜ë‹¤ëŠ” ê²ƒì„ ë°í˜”ë‹¤.

# <aside>
# ğŸ’¡ ImagebindëŠ” $(I, M)$ì˜ modality pairë¥¼ ì´ìš©í•´ trainingì„ í•œë‹¤. (I=image, M=other modality) ì´ë•Œ audio, depth, thermal, IMU (inertial measurement unit)ë¥¼ ê°ê° imageì™€ pairingí•œë‹¤.

# </aside>

# ![Untitled](assets\img\ImageBind/Untitled.png)

# Natural Alignment: $(I, M)$ê°„ì˜ alignment â†’ train
## encoders

I, Mì´ ì£¼ì–´ì¡Œì„ ë•Œ ì´ë“¤ì„ deep networkë¥¼ ì´ìš©í•´ normalized embeddingìœ¼ë¡œ encodeí•œë‹¤. Implementation & Training Detailsì—ë„ ë‚˜ì˜¤ê² ì§€ë§Œ, ì´ë•Œ encoder (f, g)ëŠ” ëª¨ë‘ Transformer.

$q_i=f(I_i)$, $k_i=g(M_i)$

## loss: InfoNCE loss

ì •í™•íˆëŠ” $L_{I, M}+L_{M,I}$ì˜ symmetric loss ì‚¬ìš©í•¨.

![Untitled](assets\img\ImageBind/Untitled%201.png)

Ï„: temparature. softmax distributionì˜ smoothness ê²°ì •

j: unrelated observation. ì¦‰ mini-batch ë‚´ì—ì„œ negative pairs ë‚˜íƒ€ëƒ„

# Emergent Alignment: $(M_1, M_2)$ ê°„ì˜ alignment â†’ evaluate

## ImagebindëŠ” ì„œë¡œ ë‹¤ë¥¸ modality pair $(M_1, M_2)$ ì˜ alignmentë¥¼ ì§ì ‘ í•™ìŠµí•˜ì§€ ì•Šê³  $(I, M_1)$, $(I, M_2)$ ë¥¼ ê°ê° í•™ìŠµí•˜ëŠ”ë°, ê·¸ë ‡ê²Œ í•´ë„ $(M_1, M_2)$ ì„ ê°™ì€ embedding spaceì— ì˜ aligní•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ í‰ê°€í•œë‹¤.

## emergent zero-shotì€ multimodal modelë“¤ì˜ emergent abilityë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ìƒˆë¡œìš´ benchmark.
# **Emergent** zero-shot classification
## modality pairë¥¼ ì§ì ‘ trainingí•˜ì§€ ì•Šê³  ê°ê°ì„ imageì™€ aligningí•˜ì—¬ trainingí•œ í›„ì— modality pair ê°„ì˜ aligningì„ í‰ê°€í•˜ëŠ” ê²ƒì„ ***emergent*** zero-shot classificationì´ë¼ê³  í•œë‹¤.
## result of Imagebind
- â€œfairâ€ baselineì€ ì—†ì§€ë§Œ, ê¸°ì¡´ì˜ audio-text aligning modelì„ ì‚¬ìš©í•˜ê±°ë‚˜, depth, thermal ê°™ì€ visual-like modalityì˜ ê²½ìš° ê·¸ëƒ¥ imageë¡œ ë³´ê³  CLIPì„ ì‚¬ìš©í•˜ê³  ì´ë¥¼ baselineì‚¼ìŒ

![Untitled](assets\img\ImageBind/Untitled%202.png)

- Text pairedê°€ ì¡´ì¬í•˜ëŠ” textì™€ pairingí•˜ì—¬ í›ˆë ¨ì‹œí‚¨ baseline. SOTAëŠ” ê° datasetì—ì„œ additional supervisionì´ë‚˜ model ensembleì„ ì¨ì„œ ì–»ì–´ë‚¸ ê°’ìœ¼ë¡œ, í•¨ê»˜ ëª…ì‹œí•¨.
# Implementation & Training Details

## <aside>
## ğŸ’¡ ë³¼ë“œì²´ ì •ë„ë§Œ ë´ë„ ë ë“¯~

## </aside>

## **Encoders (all Transformer)**
- text encoder: CLIP text encoder
- image, video â†’ same ViT (video as 2 frame image)
- audio â†’ â€˜AST: Audio Spectrogram Transformerâ€™ë¥¼ ë”°ë¼ audioë¥¼ encodingí•¨
- thermal, depth â†’ one-channel imageë¡œ ì·¨ê¸‰, ViT
- IMU â†’ ì—­ì‹œ Transformer
## Datasets
- (video, audio): Audioset
- (image, depth): SUN RGB-D
- (video, IMU): Ego4D
- (image, text)ì˜ ê²½ìš°, pretrained vision (ViT-H 630M params) and text encoders (302M params) from OpenCLIPì„ ì‚¬ìš©í–ˆìœ¼ë¯€ë¡œ, image-text supervision from large-scale web dataì¸ ì…ˆ.
## Training
- **image, text encoderëŠ” pretrained CLIPìœ¼ë¡œ initializeí•˜ê³  freezeí•¨.**
- **audio, depth, thermal, IMU encoderë§Œ updateí–ˆëŠ”ë°, ê°ê°ì„ ë”°ë¡œ í›ˆë ¨ì‹œí‚´.**
- **ê° encoderë§ˆë‹¤ modality-specific linear projectionì„ í•˜ì—¬ ê°™ì€ size dë¥¼ ê°€ì§€ëŠ” embedding ë§Œë“¤ì–´ëƒ„.**


# <aside>
# ğŸ’¡ Experiments â†’ êµ³ì´ ê¶ê¸ˆí•˜ë‹¤ë©´ ë³¼ ê²ƒ

# </aside>

# Comparison to prior works
## Zero-shot text to audio retrieval and classification

![Untitled](assets\img\ImageBind/Untitled%203.png)

- ë” ë†’ê±°ë‚˜ ë¹„ê²¬í•˜ëŠ” ê²°ê³¼
## Text to audio and video retrieval

![Untitled](assets\img\ImageBind/Untitled%204.png)

- textë¡œë¶€í„° audio, videoë¥¼ í•¨ê»˜ retrievalí•˜ëŠ” taskì—ì„œ performanceê°€ ì˜ ë‚˜ì˜¨ ê²ƒì€ pretrained OpenCLIP encoderë“¤ì„ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ìœ¼ë¡œ ë³´ì„.
- textë¡œë¶€í„° audioë§Œì„ retrievalí•˜ëŠ” taskì˜ ê²½ìš°ëŠ” ì£¼ëª©í• ë§Œí•¨. (emergent)
## Few-shot classification

![Untitled](assets\img\ImageBind/Untitled%205.png)

- ì™¼ìª½ â†’ self-supervised baselineì¸ AudioMAEë³´ë‹¨ ì¢‹ì€ ì„±ëŠ¥ì„ ê¸°ë¡í–ˆìœ¼ë©°, supervised modelë„ 4shotê¹Œì§€ëŠ” ë„˜ì–´ì„¬
- ì˜¤ë¥¸ìª½ â†’ MultiMAEë¥¼ ì™„ì „íˆ ë„˜ì–´ì„¬

# Analysis and Application
## multimodal embedding space arithmetic

![Untitled](assets\img\ImageBind/Untitled%206.png)

## upgrading text-based detectors to audio-based

â†’ object detection with audio queries

![Untitled](assets\img\ImageBind/Untitled%207.png)

## upgrading text-based diffusion models to audio-based

pretrained DALLE-2 diffusion modelì—ì„œ prompt embeddingì„ audio embeddingìœ¼ë¡œ ë°”ê¾¸ì–´ ì‹¤í—˜í•¨

![Untitled](assets\img\ImageBind/Untitled%208.png)


# CufftY (Yoonah Park)
## BIO
----------
Undergraduate Student majoring Computer Science & Engineering, Interested in Cognitive Architecture, Cellular Automata, and other DL, ML branches of study.

## Organization
----------
Seoul National University, Dept. of Computer Science & Engineering

AttentionX

## Contact
----------
[E-mail](wisdomsword21@snu.ac.kr)

[GitHub](https://github.com/gyuuuna)