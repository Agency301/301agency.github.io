---
layout: post
title:  "Visual Instruction Tuning (LLaVA)"
date:   2023-08-01 00:40:15 +0900
categories: multimodal
tags: [multimodal, vlm]
author_profile: true
author: Cufft
use_math: true
---

# Visual Instruction Tuning (LLaVA)

## Paper Review

### [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)

### Introduction
- **LLaVA: Large Language and Vision Assistant**
- nlp domain에서 주로 쓰여온 **instruction tuning**을 적용한 multimodal LLM임.
- model adaption에서의 parameter-efficiency를 향상시키는 것이 future work.
### 1. **Multimodal instruction-following data**: vision-language instruction-following data가 없다는 게 key challenge인데, data reformation perspective와 image-text pair를 appropriate instruction-following format으로 바꾸는 pipeline을 개발함
- GPT-assisted Visual Instruction Data Generation
    1. COCO dataset에서 image caption, bounding box 두 가지 유형의 symbolic representation을 만들어 image context로 사용함. 
    2. ChatGPT 및 GPT-4를 이용해 image context로부터 question-answer pair들을 생성하도록 함.
    3. `Human : Xq Xv<STOP>\n Assistant : Xc<STOP>\n`의 형태로 instruction-following data pair들을 만들어 냄. (이는 conversation, detailed description, complex reasoning의 3 type을 가짐)

    ![Untitled](https://agency301.github.io/assets/img/LLaVA/Untitled.png)

### 2. **Large multimodal models**: pretrained CLIP visual encoder와 language decoder LLaMA를 연결하고 end-to-end로 pretraining 및 fine-tuning을 하여 LLM(large multimodal model)을 구축함.
- Visual Instruction Tuning
    - Architecture

        ![Untitled](https://agency301.github.io/assets/img/LLaVA/Untitled%201.png)

        - **Vision Encoder: CLIP visual encoder ViT-L/14**
            - input image Xv는 pre-trained CLIP visual encoder ViT-L/14에서 encoding함. 이때 마지막에서 두 번째 layer output을 사용함.
        - **Projection Layer: linear layer**
            - vision encoder output을 linear layer를 거쳐 word embedding space에 mapping함 (Hv)
        - **Language Model: LLaMA 사용.**
            - 만들어낸 image feature와 language instruction을 함께 LLaMA로 넘겨 fine-tuning하며 visual instruction tuning함.
        - 이는 Q-former, gated cross-attention 같은 더 정교한 방법 대신 lightweight한 간단한 메커니즘을 사용한 것. 더 effective & sophisticated architecture는 future work.
    - Training
        - $X^t_{instruct}$는 $[X^1_q, X_v]$  또는 $[X_v, X^1_q]$  아래 그림상에서 초록색으로 표시된 부분을 generative loss 계산에 이용함

            ![Untitled](https://agency301.github.io/assets/img/LLaVA/Untitled%202.png)

            ![Untitled](https://agency301.github.io/assets/img/LLaVA/Untitled%203.png)

        - two-stage instruction-tuning procedure
            - **Stage 1: Pre-training for Feature Alignment**
                - visual encoder, LLM weights는 frozen으로 두고 projection matrix만 update
                - image feature Hv가 pre-trained LLM word embedding과 align이 되도록 해야 됨
            - **Stage 2: Fine-tuning End-to-End**
                - visual encoder를 frozen으로 두고 projection matrix와 LLM만 update
                - Multimodal Chatbot, Science QA를 case scenario로 사용함
### Experiments
- Multimodal Chatbot
    - GPT-4, BLIP-2, OpenFlamingo와 비교함. LLaVA가 훨씬 작은 dataset으로 train되었음에도 GPT-4에 비견하는 결과를 냄. BLIP-2와 OpenFlamingo는 user instruction을 잘 따르기보다는 image를 describe하는 데 더 초점을 맞추었음.
    - Quantitative Evaluation
        - GPT-4가 helpfulness, relevance, accuracy, level of details를 평가하고, 그에 대한 이유를 제시하도록 함.

            ![Untitled](https://agency301.github.io/assets/img/LLaVA/Untitled%204.png)

    - example

        ![Untitled](https://agency301.github.io/assets/img/LLaVA/Untitled%205.png)

- ScienceQA
    - GPT-3.5(text-davinci-002) with and without CoT, LLaMA-Adapter with and without MM-CoT (current SOTA), GPT-4 using 2-shot과 비교함. LLaVa가 SOTA와 quite close하게 나옴.
    - GPT-4와 LLaVa로 각각 answer만든 후 두 answer를 다시 GPT-4에 external knowledge로 주입하여 돌렸더니 new SOTA 나옴.
    - example

        ![Untitled](https://agency301.github.io/assets/img/LLaVA/Untitled%206.png)

- Ablations
    - visual encoder로 CLIP last layer output을 쓰거나 끝에서 두 번째 layer output을 쓸 때를 비교실험함. 끝에서 두 번째가 더 localized property를 보존하기 때문에 더 잘 나온다고 해석함.
    - Chain-of-thought의 contribution은 크지 않았음
    - pre-training
    - model size